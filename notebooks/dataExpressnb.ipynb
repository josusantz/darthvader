{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af6655e-ae65-417c-8189-dd9f69f5acb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings       \n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from mytools import encontrar_estacao, cortar_serie_temporal, remover_outliers_boxplot, remove_outliers_std_deviation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2e4f1d-e27c-404e-a96b-5c642c5dcb34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_A215_H_2008-06-13_2024-01-01.csv',sep= ';', header = 9)\n",
    "df['Data Medicao'] = pd.to_datetime(df['Data Medicao'])\n",
    "df['Hora Medicao'] = pd.to_datetime(df['Hora Medicao'])\n",
    "df['ano'] = df['Data Medicao'].dt.year\n",
    "df['mes'] = df['Data Medicao'].dt.month\n",
    "df['dia'] = df['Data Medicao'].dt.day\n",
    "# df['hora']= df['Hora Medicao'].str[-4:-2].astype(int)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "\n",
    "df['Hora Medicao'] = df['Hora Medicao'].astype(str)\n",
    "df['hora'] = df['Hora Medicao'].str[-4:-2].astype(int)\n",
    "df = df.drop(['Hora Medicao'],axis=1)\n",
    "df['hora'] = pd.to_timedelta(df['hora'], unit='h') - pd.Timedelta(hours=3)\n",
    "df['Data_Hora'] = df['Data Medicao'] + df['hora']\n",
    "df.set_index('Data_Hora', inplace=True)\n",
    "df['Data_Hora'] = df['Data Medicao'] + df['hora']\n",
    "df = df.drop('Unnamed: 22', axis =1)\n",
    "df['estacao'] = df.apply(encontrar_estacao, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5830fba8-d7a2-4ebe-b74e-b357bdc1f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TEMPERATURA DO PONTO DE ORVALHO(°C)','VENTO, VELOCIDADE HORARIA(m/s)', 'UMIDADE RELATIVA DO AR, HORARIA(%)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)']\n",
    "target = ['RADIACAO GLOBAL(Kj/m²)']\n",
    "keep_features = features + targetsteps=[(\"preprocessor\", preprocessor), \n",
    "           (\"model\", Rf)],\n",
    "    verbose = True\n",
    "df = df[keep_features]\n",
    "dataframe = df\n",
    "df = pd.DataFrame(df)\n",
    "X = dataframe[features]\n",
    "y = dataframe[target]\n",
    "\n",
    "class TimeSeriesCutter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, data_inicio, data_fim):\n",
    "        self.data_inicio = data_inicio\n",
    "        self.data_fim = data_fim\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return cortar_serie_temporal(df=X, data_inicio=self.data_inicio, data_fim=self.data_fim)\n",
    "\n",
    "# Custom transformer for removing outliers\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return remover_outliers_boxplot(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a02a2590-2c37-4e33-ae8f-65818eee043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, target_cols):\n",
    "        self.model = model\n",
    "        self.target_cols = target_cols\n",
    "        self.models = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Train a model for each target column\n",
    "        for col in self.target_cols:\n",
    "            self.models[col] = clone(self.model)\n",
    "            complete_cases = X.dropna(subset=[col])\n",
    "            self.models[col].fit(complete_cases.drop(columns=[col]), complete_cases[col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply the trained models to fill missing values\n",
    "        X = X.copy()\n",
    "        for col in self.target_cols:\n",
    "            missing_mask = X[col].isna()\n",
    "            X.loc[missing_mask, col] = self.models[col].predict(X.loc[missing_mask].drop(columns=self.target_cols))\n",
    "        return X\n",
    "    \n",
    "def dropna(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X.dropna()\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        return pd.DataFrame(X).dropna().values\n",
    "    else:\n",
    "        raise ValueError(\"Input data must be a pandas DataFrame or ndarray\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a77201",
   "metadata": {},
   "source": [
    "Lista de Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff36c96b-8387-49de-8c16-608635e37d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with dropna_remove_outliers\n",
      "An error occurred while processing with dropna_remove_outliers: 'numpy.ndarray' object has no attribute 'to_csv'\n",
      "Processing with dropna_no_remove_outliers\n",
      "An error occurred while processing with dropna_no_remove_outliers: 'numpy.ndarray' object has no attribute 'to_csv'\n",
      "Processing with median_remove_outliers\n",
      "An error occurred while processing with median_remove_outliers: 'numpy.ndarray' object has no attribute 'index'\n",
      "Processing with median_no_remove_outliers\n",
      "An error occurred while processing with median_no_remove_outliers: 'numpy.ndarray' object has no attribute 'index'\n",
      "Processing with linear_regression_impute_remove_outliers\n",
      "An error occurred while processing with linear_regression_impute_remove_outliers: name 'clone' is not defined\n",
      "Processing with kernel_ridge_impute_remove_outliers\n",
      "An error occurred while processing with kernel_ridge_impute_remove_outliers: name 'clone' is not defined\n",
      "Processing with random_forest_impute_remove_outliers\n",
      "An error occurred while processing with random_forest_impute_remove_outliers: name 'clone' is not defined\n",
      "Processing with svm_impute_remove_outliers\n",
      "An error occurred while processing with svm_impute_remove_outliers: name 'clone' is not defined\n",
      "Processing with knn_impute_remove_outliers\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 121\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Apply the pipeline to preprocess the data\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     df_processed \u001b[38;5;241m=\u001b[39m \u001b[43moption\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpipeline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# Save the processed data with a descriptive name\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/pipeline.py:533\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 533\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/impute/_knn.py:367\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[1;32m    359\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[1;32m    360\u001b[0m     X[row_missing_idx, :],\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[1;32m    366\u001b[0m )\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# process_chunk modifies X in place. No return value.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2172\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2171\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[0;32m-> 2172\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2174\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   2176\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2375\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m   2373\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 2375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1893\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1890\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Solarenv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:542\u001b[0m, in \u001b[0;36mnan_euclidean_distances\u001b[0;34m(X, Y, squared, missing_values, copy)\u001b[0m\n\u001b[1;32m    540\u001b[0m distances[present_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# avoid divide by zero\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresent_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresent_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m distances \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m present_count\n\u001b[1;32m    544\u001b[0m distances \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "impute_columns = ['RADIACAO GLOBAL(Kj/m²)', 'TEMPERATURA DO PONTO DE ORVALHO(°C)', 'VENTO, VELOCIDADE HORARIA(m/s)',\n",
    "                  'UMIDADE RELATIVA DO AR, HORARIA(%)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)']\n",
    "\n",
    "# Define the preprocessing choices\n",
    "preprocessing_options = [\n",
    "    {'name': 'dropna_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('dropnar',  FunctionTransformer(dropna, validate=False)), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', OutlierRemover()), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'dropna_no_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('dropnar',  FunctionTransformer(dropna, validate=False)), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', 'passthrough'), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'median_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy='median')), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', OutlierRemover()), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'median_no_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy='median')), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', 'passthrough'), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'linear_regression_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(LinearRegression(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())])},\n",
    "\n",
    "    {'name': 'kernel_ridge_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(KernelRidge(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'random_forest_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(RandomForestRegressor(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'svm_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(SVR(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'knn_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', KNNImputer(n_neighbors=3)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "    {'name': 'linear_regression_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(LinearRegression(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())])},\n",
    "\n",
    "    {'name': 'kernel_ridge_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(KernelRidge(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'random_forest_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(RandomForestRegressor(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'svm_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(SVR(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'knn_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', KNNImputer(n_neighbors=3)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])}]\n",
    "\n",
    "for option in preprocessing_options:\n",
    "    print(f\"Processing with {option['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Apply the pipeline to preprocess the data\n",
    "        df_processed = option['pipeline'].fit_transform(df)\n",
    "        \n",
    "        # Save the processed data with a descriptive name\n",
    "        filename = f\"{option['name']}.csv\"\n",
    "        df_processed.to_csv(filename, index=False)\n",
    "        \n",
    "        # Split data into features and target\n",
    "        X = df_processed['RADIACAO GLOBAL(Kj/m²)']\n",
    "        y = df_processed.drop(columns = ['RADIACAO GLOBAL(Kj/m²)'])\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize features for SVR and Neural Network\n",
    "        X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "        X_test_scaled = StandardScaler().transform(X_test)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing with {option['name']}: {e}\")\n",
    "\n",
    "\n",
    "# Iterate over each preprocessing option and apply it\n",
    "# for option in preprocessing_options:\n",
    "#     pipeline = Pipeline(option['steps'])\n",
    "    \n",
    "#     # Apply the pipeline\n",
    "#     df_processed = pipeline.fit_transform(df)\n",
    "    \n",
    "#     # Save the processed data with a descriptive name\n",
    "#     filename = f\"{option['name']}.csv\"\n",
    "#     df_processed.to_csv(filename, index=False)\n",
    "    \n",
    "    # Optionally, perform model training/testing here...\n",
    "### for option in preprocessing_pipelines:\n",
    "##     print(f\"Processing with {option['name']}\")\n",
    "# #    df_processed = option['pipeline'].fit_transform(df)\n",
    "#  #   filename = f\"{option['name']}.csv\"\n",
    "#   #  df_processed.to_csv(filename, index=False) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69486b-3b66-4bb2-951b-e33458f81916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'SVR': SVR(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse'),\n",
    "    'NeuralNetwork': Sequential([\n",
    "        Dense(64, input_dim=None, activation='relu'),  # Input dimension will be set dynamically\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Output layer\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Compile the neural network model\n",
    "models['NeuralNetwork'].compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Create a directory to save models if it does not exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "evaluation_metrics = {}\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# SVR Evaluation\n",
    "\n",
    "# Iterate over each preprocessing option\n",
    "for option in preprocessing_options:\n",
    "    print(f\"Processing with {option['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Apply the pipeline to preprocess the data\n",
    "        df_processed = option['pipeline'].fit_transform(df)\n",
    "        \n",
    "        # Save the processed data with a descriptive name\n",
    "        filename = f\"{option['name']}.csv\"\n",
    "        df_processed.to_csv(filename, index=False)\n",
    "        \n",
    "        # Split data into features and target\n",
    "        X = df_processed['RADIACAO GLOBAL(Kj/m²)']\n",
    "        y = df_processed.drop(columns = ['RADIACAO GLOBAL(Kj/m²)'])\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize features for SVR and Neural Network\n",
    "        X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "        X_test_scaled = StandardScaler().transform(X_test)\n",
    "        \n",
    "        # Update input dimension for Neural Network\n",
    "        models['NeuralNetwork'].pop(0)\n",
    "        models['NeuralNetwork'].add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "\n",
    "        # Iterate over each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Training {model_name} with {option['name']}\")\n",
    "            \n",
    "            if model_name == 'NeuralNetwork':\n",
    "                # Train the neural network\n",
    "                history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "                loss = model.evaluate(X_test_scaled, y_test)\n",
    "                print(f\"{model_name} test loss (MSE): {loss}\")\n",
    "                y_pred = model.predict(X_test_scaled).flatten()\n",
    "                \n",
    "                # Save the neural network model\n",
    "                model_save_path = os.path.join('saved_models', f\"{option['name']}_{model_name}.keras\")\n",
    "                model.save(model_save_path)\n",
    "                print(f\"Neural Network model saved as {model_save_path}\")\n",
    "            else:\n",
    "                # Define hyperparameters for model tuning\n",
    "                param_grid = {}\n",
    "                if model_name == 'SVR':\n",
    "                    param_grid = {\n",
    "                        'kernel': ['linear', 'rbf'],\n",
    "                        'C': [0.001, 0.1, 1, 10],\n",
    "                        'epsilon': [0.01, 0.1, 0.5]\n",
    "                    }\n",
    "                elif model_name == 'RandomForest':\n",
    "                    param_grid = {\n",
    "                        'n_estimators': [100, 200, 300, 1000],\n",
    "                        'max_depth': [None, 10, 20],\n",
    "                        'min_samples_split': [2, 5, 10]\n",
    "                    }\n",
    "                elif model_name == 'XGBoost':\n",
    "                    param_grid = {\n",
    "                        'max_depth': [3, 5, 7],\n",
    "                        'eta': [0.01, 0.1, 0.3],\n",
    "                        'objective': ['reg:squarederror']\n",
    "                    }\n",
    "\n",
    "                # Perform grid search if parameters are defined\n",
    "                if param_grid:\n",
    "                    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                else:\n",
    "                    best_model = model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predict and evaluate\n",
    "                y_pred = best_model.predict(X_test)\n",
    "                evaluation_metrics[model_name] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, y_pred),\n",
    "    'R²': r2_score(y_test, y_pred),\n",
    "    'EVS': explained_variance_score(y_test, y_pred)\n",
    "}\n",
    "                \n",
    "                print(f\"{model_name} - MSE: {metrics['MSE']}, \n",
    "                RMSE: {metrics['RMSE']}, MAE: {metrics['MAE']},\n",
    "                MAPE: {metrics['MAPE']}, R²: {metrics['R²']},\n",
    "                EVS: {metrics['EVS']}\")\n",
    "                \n",
    "\n",
    "                # Save the scikit-learn or XGBoost model\n",
    "                model_save_path = os.path.join('saved_models', f\"{option['name']}_{model_name}.pkl\")\n",
    "                joblib.dump(best_model, model_save_path)\n",
    "                print(f\"{model_name} model saved as {model_save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing with {option['name']}: {e}\")\n",
    "\n",
    "print(\"All preprocessing pipelines and model trainings have been executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae294e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} with {option['name']}\")\n",
    "    \n",
    "    if model_name == 'NeuralNetwork':\n",
    "        # Train the neural network\n",
    "        history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "        loss = model.evaluate(X_test_scaled, y_test)\n",
    "        print(f\"{model_name} test loss (MSE): {loss}\")\n",
    "        y_pred = model.predict(X_test_scaled).flatten()\n",
    "        \n",
    "        # Save the neural network model\n",
    "        model_save_path = os.path.join('saved_models', f\"{option['name']}_{model_name}.keras\")\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Neural Network model saved as {model_save_path}\")\n",
    "    else:\n",
    "        # Define hyperparameters for model tuning\n",
    "        param_grid = {}\n",
    "        if model_name == 'SVR':\n",
    "            param_grid = {\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'C': [0.001, 0.1, 1, 10],\n",
    "                'epsilon': [0.01, 0.1, 0.5]\n",
    "            }\n",
    "        elif model_name == 'RandomForest':\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200, 300, 1000],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        elif model_name == 'XGBoost':\n",
    "            param_grid = {\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'eta': [0.01, 0.1, 0.3],\n",
    "                'objective': ['reg:squarederror']\n",
    "            }\n",
    "\n",
    "        # Perform grid search if parameters are defined\n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "        else:\n",
    "            best_model = model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        evaluation_metrics[model_name] = {\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MAPE': mean_absolute_percentage_error(y_test, y_pred),\n",
    "            'R²': r2_score(y_test, y_pred),\n",
    "            'EVS': explained_variance_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        # Correctly formatted print statement\n",
    "        print(f\"{model_name} - MSE: {evaluation_metrics[model_name]['MSE']}, \"\n",
    "              f\"RMSE: {evaluation_metrics[model_name]['RMSE']}, MAE: {evaluation_metrics[model_name]['MAE']}, \"\n",
    "              f\"MAPE: {evaluation_metrics[model_name]['MAPE']}, R²: {evaluation_metrics[model_name]['R²']}, \"\n",
    "              f\"EVS: {evaluation_metrics[model_name]['EVS']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958819f-384b-471e-888d-b13a8ec0d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()\n",
    "# Definindo os parâmetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.001, 0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Realizando a busca em grade para encontrar os melhores parâmetros\n",
    "grid_search_svr = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros para SVR:\", grid_search_svr.best_params_)\n",
    "\n",
    "# Avaliação do modelo\n",
    "svr_best = grid_search_svr.best_estimator_\n",
    "svr_best.fit(X_train_scaled, y_train)\n",
    "svr_pred = svr_best.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"svr finalizado\")\n",
    "# Definindo o modelo Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definindo os parâmetros para GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300, 1000],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Realizando a busca em grade para encontrar os melhores parâmetros\n",
    "grid_search_rf = GridSearchCV(estimator=rf,\n",
    "                              param_grid=param_grid_rf,\n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros para Random Forest:\", \n",
    "      grid_search_rf.best_params_)\n",
    "\n",
    "# Avaliação do modelo\n",
    "rf_best = grid_search_rf.best_estimator_\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"random florest finalizado\")\n",
    "# Convertendo os dados para DMatrix para uso com XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definindo os parâmetros para XGBoost (exemplo)\n",
    "params = {\n",
    "    'max_depth': 3,drop(columns=['RADIACAO GLOBAL(Kj/m²)'])\n",
    "    'eta': 0.1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Treinamento do modelo XGBoost\n",
    "num_round = 100\n",
    "xgb_model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Avaliação do modelo\n",
    "xgb_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Construindo o modelo de RNA\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Camada de saída\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Treinamento do modelo\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "\n",
    "# Avaliação do modelo\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Erro de teste (MSE):\", loss)\n",
    "\n",
    "# Previsões\n",
    "rna_pred = model.predict(X_test_scaled).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431058ba-18dc-4d84-8306-a47625be154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Dictionary to store evaluation metrics\n",
    "evaluation_metrics = {}\n",
    "\n",
    "# SVR Evaluation\n",
    "evaluation_metrics['SVR'] = {\n",
    "    'MSE': mean_squared_error(y_test, svr_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, svr_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, svr_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, svr_pred),\n",
    "    'R²': r2_score(y_test, svr_pred),\n",
    "    'EVS': explained_variance_score(y_test, svr_pred)\n",
    "}\n",
    "\n",
    "# Random Forest Evaluation\n",
    "evaluation_metrics['Random Forest'] = {\n",
    "    'MSE': mean_squared_error(y_test, rf_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, rf_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, rf_pred),\n",
    "    'R²': r2_score(y_test, rf_pred),\n",
    "    'EVS': explained_variance_score(y_test, rf_pred)\n",
    "}\n",
    "\n",
    "# XGBoost Evaluation\n",
    "evaluation_metrics['XGBoost'] = {\n",
    "    'MSE': mean_squared_error(y_test, xgb_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, xgb_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, xgb_pred),\n",
    "    'R²': r2_score(y_test, xgb_pred),\n",
    "    'EVS': explained_variance_score(y_test, xgb_pred)\n",
    "}\n",
    "\n",
    "# RNA Evaluation\n",
    "evaluation_metrics['RNA'] = {\n",
    "    'MSE': mean_squared_error(y_test, rna_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, rna_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, rna_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, rna_pred),\n",
    "    'R²': r2_score(y_test, rna_pred),\n",
    "    'EVS': explained_variance_score(y_test, rna_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for comparison\n",
    "for model_name, metrics in evaluation_metrics.items():\n",
    "    print(f\"{model_name} - MSE: {metrics['MSE']}, \n",
    "    RMSE: {metrics['RMSE']}, MAE: {metrics['MAE']},\n",
    "    MAPE: {metrics['MAPE']}, R²: {metrics['R²']},\n",
    "    EVS: {metrics['EVS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c8a1a-4eed-4912-8dcb-099df455bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'svr_best': svr_best, 'rf_best': rf_best}\n",
    "for name, model in models.items():\n",
    "    with open(f'{name}.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "xgb_model.save_model('xgb_model.json')\n",
    "model.save('rna_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
