{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af6655e-ae65-417c-8189-dd9f69f5acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings       \n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from mytools import encontrar_estacao, cortar_serie_temporal, remover_outliers_boxplot, remove_outliers_std_deviation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2e4f1d-e27c-404e-a96b-5c642c5dcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_A215_H_2008-06-13_2024-01-01.csv',sep= ';', header = 9)\n",
    "df['Data Medicao'] = pd.to_datetime(df['Data Medicao'])\n",
    "df['Hora Medicao'] = pd.to_datetime(df['Hora Medicao'])\n",
    "df['ano'] = df['Data Medicao'].dt.year\n",
    "df['mes'] = df['Data Medicao'].dt.month\n",
    "df['dia'] = df['Data Medicao'].dt.day\n",
    "# df['hora']= df['Hora Medicao'].str[-4:-2].astype(int)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "\n",
    "df['Hora Medicao'] = df['Hora Medicao'].astype(str)\n",
    "df['hora'] = df['Hora Medicao'].str[-4:-2].astype(int)\n",
    "df = df.drop(['Hora Medicao'],axis=1)\n",
    "df['hora'] = pd.to_timedelta(df['hora'], unit='h') - pd.Timedelta(hours=3)\n",
    "df['Data_Hora'] = df['Data Medicao'] + df['hora']\n",
    "df.set_index('Data_Hora', inplace=True)\n",
    "df['Data_Hora'] = df['Data Medicao'] + df['hora']\n",
    "df = df.drop('Unnamed: 22', axis =1)\n",
    "df['estacao'] = df.apply(encontrar_estacao, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "864bf9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Data Medicao', 'PRECIPITACAO TOTAL, HORARIO(mm)',\n",
       "       'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA(mB)',\n",
       "       'PRESSAO ATMOSFERICA REDUZIDA NIVEL DO MAR, AUT(mB)',\n",
       "       'PRESSAO ATMOSFERICA MAX.NA HORA ANT. (AUT)(mB)',\n",
       "       'PRESSAO ATMOSFERICA MIN. NA HORA ANT. (AUT)(mB)',\n",
       "       'RADIACAO GLOBAL(Kj/m²)', 'TEMPERATURA DA CPU DA ESTACAO(°C)',\n",
       "       'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)',\n",
       "       'TEMPERATURA DO PONTO DE ORVALHO(°C)',\n",
       "       'TEMPERATURA MAXIMA NA HORA ANT. (AUT)(°C)',\n",
       "       'TEMPERATURA MINIMA NA HORA ANT. (AUT)(°C)',\n",
       "       'TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT)(°C)',\n",
       "       'TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT)(°C)',\n",
       "       'TENSAO DA BATERIA DA ESTACAO(V)',\n",
       "       'UMIDADE REL. MAX. NA HORA ANT. (AUT)(%)',\n",
       "       'UMIDADE REL. MIN. NA HORA ANT. (AUT)(%)',\n",
       "       'UMIDADE RELATIVA DO AR, HORARIA(%)',\n",
       "       'VENTO, DIRECAO HORARIA (gr)(° (gr))', 'VENTO, RAJADA MAXIMA(m/s)',\n",
       "       'VENTO, VELOCIDADE HORARIA(m/s)', 'ano', 'mes', 'dia', 'hora',\n",
       "       'Data_Hora', 'estacao'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9b5ea-2d39-405a-b4b5-527037af5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "sns.scatterplot(x='Data Medicao',y=\"RADIACAO GLOBAL(Kj/m²)\", data= df)\n",
    "# plt.savefig('radiacao.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f6396-b4d1-490f-8167-f138e440f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df_crop = cortar_serie_temporal(df = df,data_inicio = '2010-01-01', data_fim = '2020-12-31')\n",
    "\n",
    "df = remover_outliers(df_crop)\n",
    "df.to_csv('cropped.csv')\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "features = ['TEMPERATURA DO PONTO DE ORVALHO(°C)','VENTO, VELOCIDADE HORARIA(m/s)', 'UMIDADE RELATIVA DO AR, HORARIA(%)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)']\n",
    "target = ['RADIACAO GLOBAL(Kj/m²)']\n",
    "dataframe = df\n",
    "X = dataframe[features].values\n",
    "y = dataframe[target].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scalfrom sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_spliter.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5e3a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_features = features + target\n",
    "df = df[keep_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5830fba8-d7a2-4ebe-b74e-b357bdc1f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TEMPERATURA DO PONTO DE ORVALHO(°C)','VENTO, VELOCIDADE HORARIA(m/s)', 'UMIDADE RELATIVA DO AR, HORARIA(%)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)']\n",
    "target = ['RADIACAO GLOBAL(Kj/m²)']\n",
    "dataframe = df\n",
    "X = dataframe[features].values\n",
    "y = dataframe[target].values\n",
    "\n",
    "class TimeSeriesCutter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, data_inicio, data_fim):\n",
    "        self.data_inicio = data_inicio\n",
    "        self.data_fim = data_fim\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return cortar_serie_temporal(df=X, data_inicio=self.data_inicio, data_fim=self.data_fim)\n",
    "\n",
    "# Custom transformer for removing outliers\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return remover_outliers_boxplot(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563063d-94ce-4f43-84ec-5377e828a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinearRegressionImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.complete_cases = X.dropna(subset=[self.target_col])\n",
    "        self.model.fit(self.complete_cases.drop(columns=[self.target_col]), self.complete_cases[self.target_col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        missing_mask = X[self.target_col].isna()\n",
    "        X.loc[missing_mask, self.target_col] = self.model.predict(X.loc[missing_mask].drop(columns=[self.target_col]))\n",
    "        return X\n",
    "\n",
    "class KernelRidgeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        self.model = KernelRidge()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.complete_cases = X.dropna(subset=[self.target_col])\n",
    "        self.model.fit(self.complete_cases.drop(columns=[self.target_col]), self.complete_cases[self.target_col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        missing_mask = X[self.target_col].isna()\n",
    "        X.loc[missing_mask, self.target_col] = self.model.predict(X.loc[missing_mask].drop(columns=[self.target_col]))\n",
    "        return X\n",
    "\n",
    "class RandomForestImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        self.model = RandomForestRegressor()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.complete_cases = X.dropna(subset=[self.target_col])\n",
    "        self.model.fit(self.complete_cases.drop(columns=[self.target_col]), self.complete_cases[self.target_col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        missing_mask = X[self.target_col].isna()\n",
    "        X.loc[missing_mask, self.target_col] = self.model.predict(X.loc[missing_mask].drop(columns=[self.target_col]))\n",
    "        return X\n",
    "\n",
    "class SVMImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col):\n",
    "        self.target_col = target_col\n",
    "        self.model = SVR()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.complete_cases = X.dropna(subset=[self.target_col])\n",
    "        self.model.fit(self.complete_cases.drop(columns=[self.target_col]), self.complete_cases[self.target_col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        missing_mask = X[self.target_col].isna()\n",
    "        X.loc[missing_mask, self.target_col] = self.model.predict(X.loc[missing_mask].drop(columns=[self.target_col]))\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02a2590-2c37-4e33-ae8f-65818eee043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, target_cols):\n",
    "        self.model = model\n",
    "        self.target_cols = target_cols\n",
    "        self.models = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Train a model for each target column\n",
    "        for col in self.target_cols:\n",
    "            self.models[col] = clone(self.model)\n",
    "            complete_cases = X.dropna(subset=[col])\n",
    "            self.models[col].fit(complete_cases.drop(columns=[col]), complete_cases[col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply the trained models to fill missing values\n",
    "        X = X.copy()\n",
    "        for col in self.target_cols:\n",
    "            missing_mask = X[col].isna()\n",
    "            X.loc[missing_mask, col] = self.models[col].predict(X.loc[missing_mask].drop(columns=self.target_cols))\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043a22f-a7cd-44cd-bfd9-f45905290abc",
   "metadata": {},
   "source": [
    "# Lista de pipelines para gerenciamento do projeto onde temos a remoção ou imputação dos valores nulos, assim como a remoção ou não de outliers # \n",
    "## Lembrete: Após avaliar o desempesnho dos modelos com os pipelines inserir a remoção ou não dos valores noturnos ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff36c96b-8387-49de-8c16-608635e37d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "impute_columns = ['RADIACAO GLOBAL(Kj/m²)', 'TEMPERATURA DO PONTO DE ORVALHO(°C)', 'VENTO, VELOCIDADE HORARIA(m/s)',\n",
    "                  'UMIDADE RELATIVA DO AR, HORARIA(%)', 'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)']\n",
    "\n",
    "# Define the preprocessing choices\n",
    "preprocessing_options = [\n",
    "    {'name': 'dropna_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('dropnar', 'drop'), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', OutlierRemover()), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'dropna_no_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('dropnar', 'drop'), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', 'passthrough'), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'median_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy='median')), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', OutlierRemover()), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'median_no_remove_outliers', \n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy='median')), \n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')), \n",
    "         ('remove_outliers', 'passthrough'), \n",
    "         ('scaler', StandardScaler())])},\n",
    "    \n",
    "    {'name': 'linear_regression_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(LinearRegression(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())])},\n",
    "\n",
    "    {'name': 'kernel_ridge_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(KernelRidge(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'random_forest_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(RandomForestRegressor(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'svm_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(SVR(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'knn_impute_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', KNNImputer(n_neighbors=3)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', OutlierRemover()),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "    {'name': 'linear_regression_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(LinearRegression(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())])},\n",
    "\n",
    "    {'name': 'kernel_ridge_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(KernelRidge(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'random_forest_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(RandomForestRegressor(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'svm_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', MultiColumnImputer(SVR(), target_cols=impute_columns)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])},\n",
    "\n",
    "    {'name': 'knn_impute_no_remove_outliers',\n",
    "     'pipeline': Pipeline([\n",
    "         ('imputer', KNNImputer(n_neighbors=3)),\n",
    "         ('cut_series', TimeSeriesCutter(data_inicio='2010-01-01', data_fim='2020-12-31')),\n",
    "         ('remove_outliers', 'passthrough'),\n",
    "         ('scaler', StandardScaler())\n",
    "     ])}]\n",
    "\n",
    "# Iterate over each preprocessing option and apply it\n",
    "# for option in preprocessing_options:\n",
    "#     pipeline = Pipeline(option['steps'])\n",
    "    \n",
    "#     # Apply the pipeline\n",
    "#     df_processed = pipeline.fit_transform(df)\n",
    "    \n",
    "#     # Save the processed data with a descriptive name\n",
    "#     filename = f\"{option['name']}.csv\"\n",
    "#     df_processed.to_csv(filename, index=False)\n",
    "    \n",
    "    # Optionally, perform model training/testing here...\n",
    "### for option in preprocessing_pipelines:\n",
    "##     print(f\"Processing with {option['name']}\")\n",
    "# #    df_processed = option['pipeline'].fit_transform(df)\n",
    "#  #   filename = f\"{option['name']}.csv\"\n",
    "#   #  df_processed.to_csv(filename, index=False) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69486b-3b66-4bb2-951b-e33458f81916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'SVR': SVR(),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse'),\n",
    "    'NeuralNetwork': Sequential([\n",
    "        Dense(64, input_dim=None, activation='relu'),  # Input dimension will be set dynamically\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Output layer\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Compile the neural network model\n",
    "models['NeuralNetwork'].compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Create a directory to save models if it does not exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "evaluation_metrics = {}\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# SVR Evaluation\n",
    "\n",
    "# Iterate over each preprocessing option\n",
    "for option in preprocessing_options:\n",
    "    print(f\"Processing with {option['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Apply the pipeline to preprocess the data\n",
    "        df_processed = option['pipeline'].fit_transform(df)\n",
    "        \n",
    "        # Save the processed data with a descriptive name\n",
    "        filename = f\"{option['name']}.csv\"\n",
    "        df_processed.to_csv(filename, index=False)\n",
    "        \n",
    "        # Split data into features and target\n",
    "        X = df_processed['RADIACAO GLOBAL(Kj/m²)']\n",
    "        y = df_processed.drop(columns = ['RADIACAO GLOBAL(Kj/m²)'])\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Standardize features for SVR and Neural Network\n",
    "        X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "        X_test_scaled = StandardScaler().transform(X_test)\n",
    "        \n",
    "        # Update input dimension for Neural Network\n",
    "        models['NeuralNetwork'].pop(0)\n",
    "        models['NeuralNetwork'].add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "\n",
    "        # Iterate over each model\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Training {model_name} with {option['name']}\")\n",
    "            \n",
    "            if model_name == 'NeuralNetwork':\n",
    "                # Train the neural network\n",
    "                history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "                loss = model.evaluate(X_test_scaled, y_test)\n",
    "                print(f\"{model_name} test loss (MSE): {loss}\")\n",
    "                y_pred = model.predict(X_test_scaled).flatten()\n",
    "                \n",
    "                # Save the neural network model\n",
    "                model_save_path = os.path.join('saved_models', f\"{option['name']}_{model_name}.keras\")\n",
    "                model.save(model_save_path)\n",
    "                print(f\"Neural Network model saved as {model_save_path}\")\n",
    "            else:\n",
    "                # Define hyperparameters for model tuning\n",
    "                param_grid = {}\n",
    "                if model_name == 'SVR':\n",
    "                    param_grid = {\n",
    "                        'kernel': ['linear', 'rbf'],\n",
    "                        'C': [0.001, 0.1, 1, 10],\n",
    "                        'epsilon': [0.01, 0.1, 0.5]\n",
    "                    }\n",
    "                elif model_name == 'RandomForest':\n",
    "                    param_grid = {\n",
    "                        'n_estimators': [100, 200, 300, 1000],\n",
    "                        'max_depth': [None, 10, 20],\n",
    "                        'min_samples_split': [2, 5, 10]\n",
    "                    }\n",
    "                elif model_name == 'XGBoost':\n",
    "                    param_grid = {\n",
    "                        'max_depth': [3, 5, 7],\n",
    "                        'eta': [0.01, 0.1, 0.3],\n",
    "                        'objective': ['reg:squarederror']\n",
    "                    }\n",
    "\n",
    "                # Perform grid search if parameters are defined\n",
    "                if param_grid:\n",
    "                    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                else:\n",
    "                    best_model = model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predict and evaluate\n",
    "                y_pred = best_model.predict(X_test)\n",
    "                evaluation_metrics[model_name] = {\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, y_pred),\n",
    "    'R²': r2_score(y_test, y_pred),\n",
    "    'EVS': explained_variance_score(y_test, y_pred)\n",
    "}\n",
    "                \n",
    "                print(f\"{model_name} - MSE: {metrics['MSE']}, \n",
    "                RMSE: {metrics['RMSE']}, MAE: {metrics['MAE']},\n",
    "                MAPE: {metrics['MAPE']}, R²: {metrics['R²']},\n",
    "                EVS: {metrics['EVS']}\")\n",
    "                \n",
    "\n",
    "                # Save the scikit-learn or XGBoost model\n",
    "                model_save_path = os.path.join('saved_models', f\"{option['name']}_{model_name}.pkl\")\n",
    "                joblib.dump(best_model, model_save_path)\n",
    "                print(f\"{model_name} model saved as {model_save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing with {option['name']}: {e}\")\n",
    "\n",
    "print(\"All preprocessing pipelines and model trainings have been executed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae294e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} with {option['name']}\")\n",
    "    \n",
    "    if model_name == 'NeuralNetwork':\n",
    "        # Train the neural network\n",
    "        history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "        loss = model.evaluate(X_test_scaled, y_test)\n",
    "        print(f\"{model_name} test loss (MSE): {loss}\")\n",
    "        y_pred = model.predict(X_test_scaled).flatten()\n",
    "        \n",
    "        # Save the neural network model\n",
    "        model_save_path = os.path.join('saved_models', f\"{option['name']}_{model_name}.keras\")\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Neural Network model saved as {model_save_path}\")\n",
    "    else:\n",
    "        # Define hyperparameters for model tuning\n",
    "        param_grid = {}\n",
    "        if model_name == 'SVR':\n",
    "            param_grid = {\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'C': [0.001, 0.1, 1, 10],\n",
    "                'epsilon': [0.01, 0.1, 0.5]\n",
    "            }\n",
    "        elif model_name == 'RandomForest':\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200, 300, 1000],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        elif model_name == 'XGBoost':\n",
    "            param_grid = {\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'eta': [0.01, 0.1, 0.3],\n",
    "                'objective': ['reg:squarederror']\n",
    "            }\n",
    "\n",
    "        # Perform grid search if parameters are defined\n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "        else:\n",
    "            best_model = model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        evaluation_metrics[model_name] = {\n",
    "            'MSE': mean_squared_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'MAPE': mean_absolute_percentage_error(y_test, y_pred),\n",
    "            'R²': r2_score(y_test, y_pred),\n",
    "            'EVS': explained_variance_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        # Correctly formatted print statement\n",
    "        print(f\"{model_name} - MSE: {evaluation_metrics[model_name]['MSE']}, \"\n",
    "              f\"RMSE: {evaluation_metrics[model_name]['RMSE']}, MAE: {evaluation_metrics[model_name]['MAE']}, \"\n",
    "              f\"MAPE: {evaluation_metrics[model_name]['MAPE']}, R²: {evaluation_metrics[model_name]['R²']}, \"\n",
    "              f\"EVS: {evaluation_metrics[model_name]['EVS']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958819f-384b-471e-888d-b13a8ec0d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()\n",
    "# Definindo os parâmetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.001, 0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Realizando a busca em grade para encontrar os melhores parâmetros\n",
    "grid_search_svr = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros para SVR:\", grid_search_svr.best_params_)\n",
    "\n",
    "# Avaliação do modelo\n",
    "svr_best = grid_search_svr.best_estimator_\n",
    "svr_best.fit(X_train_scaled, y_train)\n",
    "svr_pred = svr_best.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(\"svr finalizado\")\n",
    "# Definindo o modelo Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definindo os parâmetros para GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300, 1000],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Realizando a busca em grade para encontrar os melhores parâmetros\n",
    "grid_search_rf = GridSearchCV(estimator=rf,\n",
    "                              param_grid=param_grid_rf,\n",
    "                              cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros para Random Forest:\", \n",
    "      grid_search_rf.best_params_)\n",
    "\n",
    "# Avaliação do modelo\n",
    "rf_best = grid_search_rf.best_estimator_\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"random florest finalizado\")\n",
    "# Convertendo os dados para DMatrix para uso com XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definindo os parâmetros para XGBoost (exemplo)\n",
    "params = {\n",
    "    'max_depth': 3,drop(columns=['RADIACAO GLOBAL(Kj/m²)'])\n",
    "    'eta': 0.1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Treinamento do modelo XGBoost\n",
    "num_round = 100\n",
    "xgb_model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Avaliação do modelo\n",
    "xgb_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Construindo o modelo de RNA\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Camada de saída\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Treinamento do modelo\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "\n",
    "# Avaliação do modelo\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Erro de teste (MSE):\", loss)\n",
    "\n",
    "# Previsões\n",
    "rna_pred = model.predict(X_test_scaled).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431058ba-18dc-4d84-8306-a47625be154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Dictionary to store evaluation metrics\n",
    "evaluation_metrics = {}\n",
    "\n",
    "# SVR Evaluation\n",
    "evaluation_metrics['SVR'] = {\n",
    "    'MSE': mean_squared_error(y_test, svr_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, svr_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, svr_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, svr_pred),\n",
    "    'R²': r2_score(y_test, svr_pred),\n",
    "    'EVS': explained_variance_score(y_test, svr_pred)\n",
    "}\n",
    "\n",
    "# Random Forest Evaluation\n",
    "evaluation_metrics['Random Forest'] = {\n",
    "    'MSE': mean_squared_error(y_test, rf_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, rf_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, rf_pred),\n",
    "    'R²': r2_score(y_test, rf_pred),\n",
    "    'EVS': explained_variance_score(y_test, rf_pred)\n",
    "}\n",
    "\n",
    "# XGBoost Evaluation\n",
    "evaluation_metrics['XGBoost'] = {\n",
    "    'MSE': mean_squared_error(y_test, xgb_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, xgb_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, xgb_pred),\n",
    "    'R²': r2_score(y_test, xgb_pred),\n",
    "    'EVS': explained_variance_score(y_test, xgb_pred)\n",
    "}\n",
    "\n",
    "# RNA Evaluation\n",
    "evaluation_metrics['RNA'] = {\n",
    "    'MSE': mean_squared_error(y_test, rna_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, rna_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, rna_pred),\n",
    "    'MAPE': mean_absolute_percentage_error(y_test, rna_pred),\n",
    "    'R²': r2_score(y_test, rna_pred),\n",
    "    'EVS': explained_variance_score(y_test, rna_pred)\n",
    "}\n",
    "\n",
    "# Print evaluation metrics for comparison\n",
    "for model_name, metrics in evaluation_metrics.items():\n",
    "    print(f\"{model_name} - MSE: {metrics['MSE']}, \n",
    "    RMSE: {metrics['RMSE']}, MAE: {metrics['MAE']},\n",
    "    MAPE: {metrics['MAPE']}, R²: {metrics['R²']},\n",
    "    EVS: {metrics['EVS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c8a1a-4eed-4912-8dcb-099df455bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'svr_best': svr_best, 'rf_best': rf_best}\n",
    "for name, model in models.items():\n",
    "    with open(f'{name}.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "xgb_model.save_model('xgb_model.json')\n",
    "model.save('rna_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
